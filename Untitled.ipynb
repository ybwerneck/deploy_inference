{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cf0213b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.no_grad at 0x7f8041cc8b50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as pt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "import collections as coll\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import numpy as np\n",
    "import torch as pt\n",
    "from typing import TypeVar\n",
    "    \n",
    "pt.set_grad_enabled (False) \n",
    "numinputs=2\n",
    "numoutputs=2\n",
    "\n",
    "T_module = TypeVar('T_module', bound=nn.Module)\n",
    "\n",
    "\n",
    "\n",
    "class net(nn.Module):\n",
    "          \n",
    "            \n",
    "    def __init__(self, numinputs, numoutputs,H=10):\n",
    "        super(net, self).__init__()\n",
    "        heigth=H\n",
    "        \n",
    "        self.layer0 = nn.utils.weight_norm(nn.Linear(numinputs, heigth),name='weight', dim=0).cuda()\n",
    "        self.layer1 = nn.utils.weight_norm(nn.Linear(heigth, heigth),name='weight', dim=0).cuda()\n",
    "        self.layer2 = nn.utils.weight_norm(nn.Linear(heigth, heigth),name='weight', dim=0).cuda()\n",
    "        self.layer3 = nn.utils.weight_norm(nn.Linear(heigth, heigth),name='weight', dim=0).cuda()\n",
    "\n",
    "        self.layer10 = nn.Linear(heigth, numoutputs).cuda()\n",
    "\n",
    "        self.layer0.eval()\n",
    "        self.layer1.eval()\n",
    "        self.layer2.eval()\n",
    "        self.layer3.eval()\n",
    "        self.layer10.eval()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x0 = f.silu(self.layer0(x))\n",
    "        x1 = f.silu(self.layer1(x0))\n",
    "        x2 = f.silu(self.layer2(x1))\n",
    "        x3 = f.silu(self.layer3(x2))\n",
    " \n",
    " \n",
    "        return self.layer10(x3)\n",
    "    \n",
    "    def load(self, od):\n",
    "        for k, v in od.items():\n",
    "            print(k,\"loaded as\")\n",
    "            if(k == '_impl.layers.0.linear.weight'): \n",
    "        #        print('Loading: ',k, np.shape(v), ' -> self.layer0.weight.data')\n",
    "                self.layer0.weight_v.data = v\n",
    "                self.layer0.weight_v.requires_grad = False\n",
    "            if(k == '_impl.layers.0.linear.weight_g'):\n",
    "       #         print('Loading: ',k, np.shape(v), ' -> self.layer0.weight_g.data')\n",
    "                self.layer0.weight_g.data = v\n",
    "                self.layer0.weight_g.requires_grad = False\n",
    "            if(k == '_impl.layers.0.linear.bias'):\n",
    "      #          print('Loading: ',k, np.shape(v), ' -> self.layer0.bias.data')\n",
    "                self.layer0.bias.data = v\n",
    "                self.layer0.bias.requires_grad = False\n",
    "            if(k == '_impl.layers.1.linear.weight'):\n",
    "     #           print('Loading: ',k, np.shape(v), ' -> self.layer1.weight.data')\n",
    "                self.layer1.weight_v.data = v\n",
    "                self.layer1.weight_v.requires_grad = False\n",
    "            if(k == '_impl.layers.1.linear.weight_g'):\n",
    "    #            print('Loading: ',k, np.shape(v), ' -> self.layer1.weight_g.data')\n",
    "                self.layer1.weight_g.data = v\n",
    "                self.layer1.weight_g.requires_grad = False\n",
    "            if(k == '_impl.layers.1.linear.bias'):\n",
    "   #             print('Loading: ',k, np.shape(v), ' -> self.layer1.bias.data')\n",
    "                self.layer1.bias.data = v\n",
    "                self.layer1.weight_g.requires_grad = False\n",
    "            if(k == '_impl.layers.2.linear.weight'):\n",
    "         #       print('Loading: ',k, np.shape(v), ' -> self.layer2.weight.data')\n",
    "                self.layer2.weight_v.data = v\n",
    "                self.layer2.weight_v.requires_grad = False\n",
    "            if(k == '_impl.layers.2.linear.weight_g'):\n",
    "  #              print('Loading: ',k, np.shape(v), ' -> self.layer2.weight_g.data')\n",
    "                self.layer2.weight_g.data = v\n",
    "                self.layer2.weight_g.requires_grad = False\n",
    "            if(k == '_impl.layers.2.linear.bias'):\n",
    " #               print('Loading: ',k, np.shape(v), ' -> self.layer2.bias.data')\n",
    "                self.layer2.bias.data = v\n",
    "                self.layer2.bias.requires_grad = False\n",
    "                \n",
    "                \n",
    "            if(k == '_impl.layers.3.linear.weight'):\n",
    "#                print('Loading: ',k, np.shape(v), ' -> self.layer3.weight.data')\n",
    "                self.layer3.weight_v.data = v\n",
    "                self.layer3.weight_v.requires_grad = False\n",
    "            if(k == '_impl.layers.3.linear.weight_g'):\n",
    "       #         print('Loading: ',k, np.shape(v), ' -> self.layer3.weight_g.data')\n",
    "                self.layer3.weight_g.data = v\n",
    "                self.layer3.weight_g.requires_grad = False\n",
    "            if(k == '_impl.layers.3.linear.bias'):\n",
    "      #          print('Loading: ',k, np.shape(v), ' -> self.layer3.bias.data')\n",
    "                self.layer3.bias.data = v\n",
    "                self.layer3.bias.requires_grad = False  \n",
    "                \n",
    "                \n",
    "            if(k == '_impl.final_layer.linear.weight'):\n",
    "     #           print('Loading: ',k, np.shape(v), ' -> self.layer10.weight.data')\n",
    "                self.layer10.weight.data = v\n",
    "                self.layer10.weight.requires_grad = False\n",
    "            if(k == '_impl.final_layer.linear.bias'):\n",
    "    #            print('Loading: ',k, np.shape(v), ' -> self.layer10.bias.data')\n",
    "                self.layer10.bias.data = v\n",
    "                self.layer10.bias.requires_grad = False\n",
    "    \n",
    "    def __prepare_scriptable__(self):\n",
    "        for layer in [self.layer0,self.layer1,self.layer2,self.layer3]:\n",
    "            for hook in layer._forward_pre_hooks.values():\n",
    "                print(hook)\n",
    "                # The hook we want to remove is an instance of WeightNorm class, so\n",
    "                # normally we would do `if isinstance(...)` but this class is not accessible\n",
    "                # because of shadowing, so we check the module name directly.\n",
    "                # https://github.com/pytorch/pytorch/blob/be0ca00c5ce260eb5bcec3237357f7a30cc08983/torch/nn/utils/__init__.py#L3\n",
    "                if hook.__module__ == \"torch.nn.utils.weight_norm\" and hook.__class__.__name__ == \"WeightNorm\":\n",
    "                    #_LG.warning(\"Removing weight_norm from %s\", self.__class__.__name__)\n",
    "                    torch.nn.utils.remove_weight_norm(layer)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, numinputs, numoutputs, numlayers=4, H=10):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.utils.weight_norm(nn.Linear(numinputs, H), name='weight', dim=0).cuda())\n",
    "\n",
    "        for _ in range(numlayers - 1):\n",
    "            self.layers.append(nn.utils.weight_norm(nn.Linear(H, H), name='weight', dim=0).cuda())\n",
    "\n",
    "        self.final_layer = nn.Linear(H, numoutputs).cuda()\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.eval()\n",
    "        self.final_layer.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = f.silu(layer(x))\n",
    "\n",
    "        return self.final_layer(x)\n",
    "\n",
    "    def load(self, od):\n",
    "        for k, v in od.items():\n",
    "            if k.startswith('_impl.layers'):\n",
    "                layer_num = int(k.split('.')[2])\n",
    "                layer = self.layers[layer_num]\n",
    "                if k.endswith('linear.weight'):\n",
    "                    layer.weight_v.data = v\n",
    "                    layer.weight_v.requires_grad = False\n",
    "                elif k.endswith('linear.weight_g'):\n",
    "                    layer.weight_g.data = v\n",
    "                    layer.weight_g.requires_grad = False\n",
    "                elif k.endswith('linear.bias'):\n",
    "                    layer.bias.data = v\n",
    "                    layer.bias.requires_grad = False\n",
    "            elif k == '_impl.final_layer.linear.weight':\n",
    "                self.final_layer.weight.data = v\n",
    "                self.final_layer.weight.requires_grad = False\n",
    "            elif k == '_impl.final_layer.linear.bias':\n",
    "                self.final_layer.bias.data = v\n",
    "                self.final_layer.bias.requires_grad = False\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        for layer in self.layers:\n",
    "            for hook in layer._forward_pre_hooks.values():\n",
    "                if hook.__module__ == \"torch.nn.utils.weight_norm\" and hook.__class__.__name__ == \"WeightNorm\":\n",
    "                    torch.nn.utils.remove_weight_norm(layer)\n",
    "        return self\n",
    "    \n",
    "model=Net(2,2)\n",
    "PATH=\"\"\n",
    "od=pt.load('outputs/fhn1P/initial_conditions/network.0.pth')\n",
    "model.load(od)\n",
    "pt.no_grad() \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab66e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as pt\n",
    "import torch\n",
    "import itertools\n",
    "import time as TIME\n",
    "import torch, gc\n",
    "\n",
    "\n",
    "def eval(M):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    start_time = TIME.time()\n",
    "\n",
    "    my2dspace = pt.tensor(X.T, requires_grad=False).float().cuda()\n",
    "    \n",
    "    reftime = TIME.perf_counter()- start_time\n",
    "    \n",
    "    #print(\"transfer time\",reftime)\n",
    "    #print(np.shape(X))\n",
    "    M.eval()\n",
    "    my2dspace = pt.tensor(X.T, requires_grad=False).float().cuda()\n",
    "   sudo dnf install nvidia-docker2\n",
    "\n",
    "    start_time = TIME.perf_counter()\n",
    "\n",
    "\n",
    "    myOutput = M(my2dspace)\n",
    "    \n",
    "    reftime = TIME.perf_counter()- start_time\n",
    "   \n",
    "    #print(np.shape(uu))\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return reftime\n",
    "\n",
    "\n",
    "#mod = pt.jit.script(model)\n",
    "#models=(Net(2,2,numlayers=2,H=2),Net(2,2,numlayers=5,H=5),Net(2,2,numlayers=7,H=7) )\n",
    "\n",
    "NL=[4,8,16,32]\n",
    "HS=[4,8,16,32,64]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "for N in [10,1000,10000,1000000,10000000]:\n",
    "    t=np.linspace(0,10,50)\n",
    "    x=np.linspace(0,1,N)\n",
    "    X=np.zeros((2,len(t)*len(x)))\n",
    "        #print(itertools.product(x,t))\n",
    "    i=0\n",
    "    for a,b in itertools.product(x,t):\n",
    "            X[:,i]=(b,a)\n",
    "            i=i+1\n",
    "    plt.figure(figsize=(10, 8)) \n",
    "    for nl in NL:\n",
    "        ts=np.zeros(len(HS))\n",
    "        i=0\n",
    "        for h in HS:\n",
    "            ts[i]=np.mean([eval(Net(2,2,numlayers=nl,H=h)) for _ in range(10)])\n",
    "            i=i+1\n",
    "\n",
    "        plt.plot(HS,ts,label=\"N_l=\"+str(nl))\n",
    "        print(str(nl),\"a\",ts)\n",
    "        ts=np.zeros(len(HS))\n",
    "        i=0\n",
    "        for h in HS:\n",
    "            ts[i]=np.mean([eval(torch.jit.script(model).cuda()) for _ in range(10)])\n",
    "            i=i+1\n",
    "\n",
    "        plt.plot(HS,ts,\"--\",label=\"Nscript_l=\"+str(nl))\n",
    "        print(str(nl),\"b\",ts)\n",
    "\n",
    "    plt.xlabel('Height of Each Layer')\n",
    "    plt.ylabel('Time of Evaluation')\n",
    "    plt.title('Evaluation Time vs. Number of Layers n='+str(N))\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa2d0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Number of rows in the CSV file:  10 51\n",
      "Solving for  10 cells in  51 timepoints\n",
      "Shape ref  (510,)\n",
      "ref cuda time [0.00014025599999999998, 0.021213184, 0.000444896]\n",
      "2\n",
      "Number of rows in the CSV file:  10 51\n",
      "Shape cudapred  (510,)\n",
      "cuda time [0.00011248, 0.00021376, 0.000252096]\n",
      "Error Calculation\n",
      "mean 0.0004925686274509804\n",
      "max 0.011299000000000003\n",
      "(52,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Modelrun' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mshape(t))\n\u001b[1;32m     41\u001b[0m start_time \u001b[38;5;241m=\u001b[39m TIME\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 42\u001b[0m u,net_time\u001b[38;5;241m=\u001b[39m\u001b[43mModelrun\u001b[49m(x0,t[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m     44\u001b[0m u_net\u001b[38;5;241m=\u001b[39mu\u001b[38;5;241m.\u001b[39mT[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape netpred \u001b[39m\u001b[38;5;124m\"\u001b[39m,np\u001b[38;5;241m.\u001b[39mshape(u_net))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Modelrun' is not defined"
     ]
    }
   ],
   "source": [
    "from FHNCUDAlib import FHNCUDA\n",
    "import numpy as np\n",
    "\n",
    "import time as TIME\n",
    "x0=np.expand_dims(np.array([(0.5 + 0.1*i*1) for i in range(0,10)]),-1)\n",
    "dt,tt=0.0001,10\n",
    "\n",
    "rate=2000\n",
    "\n",
    "start_time = TIME.time()\n",
    "u,v ,t,p=FHNCUDA.run(x0,tt,dt,rate)\n",
    "reftime = TIME.time()- start_time\n",
    "p=[i/1000 for i in p[0]]\n",
    "u_ref=np.array(u).flatten()\n",
    "\n",
    "\n",
    "print(\"Solving for \",len(x0),\"cells in \" ,int(np.shape(u_ref)[0]/len(x0)),\"timepoints\" )\n",
    "print(\"Shape ref \",np.shape(u_ref))\n",
    "print(\"ref cuda time\",p)\n",
    "\n",
    "\n",
    "\n",
    "start_time = TIME.time()\n",
    "u,v ,t,p=FHNCUDA.run(x0,tt,dt*100,rate/100)\n",
    "cudatime = TIME.time()- start_time\n",
    "u_num=np.array(u).flatten()\n",
    "#print(np.unique(t))\n",
    "p=[i/1000 for i in p[0]]\n",
    "print(\"Shape cudapred \",np.shape(u_num))\n",
    "print(\"cuda time\",p)\n",
    "print(\"Error Calculation\")\n",
    "e=((u_ref-u_num)**2)**(1/2)\n",
    "print(\"mean\",np.mean(e))\n",
    "m=np.max(e)\n",
    "print(\"max\",m)\n",
    "i=[a for a in range(len(e)) if e[a]==m]\n",
    "x0 = [item for sublist in x0 for item in sublist]\n",
    "t = [item for sublist in t for item in sublist]\n",
    "\n",
    "print(np.shape(t))\n",
    "start_time = TIME.time()\n",
    "u,net_time=Modelrun(x0,t[1:])\n",
    "\n",
    "u_net=u.T[0].flatten()\n",
    "\n",
    "print(\"Shape netpred \",np.shape(u_net))\n",
    "print(\"net time\", net_time)\n",
    "print(\"Error Calculation\")\n",
    "e=((u_ref-u_net)**2)**(1/2)\n",
    "print(\"mean\",np.mean(e))\n",
    "m=np.max(e)\n",
    "print(\"max\",m)\n",
    "i=[a for a in range(len(e)) if e[a]==m]\n",
    "\n",
    "\n",
    "plt.plot(u_ref,\"b\")\n",
    "plt.plot(u_net,\"r\")\n",
    "plt.savefig(\"aa.png\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "e=lambda x:np.array(x).T\n",
    "\n",
    "invar={\"t\":e(t )  ,\"K\":e(x0)   }\n",
    "out={\"x1\":e(u_net),       }\n",
    "out_t={\"x1\":e(u_num),         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23984227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin compiling\n",
      "<torch.nn.utils.weight_norm.WeightNorm object at 0x7f7fe55483a0>\n",
      "<torch.nn.utils.weight_norm.WeightNorm object at 0x7f803470d2d0>\n",
      "<torch.nn.utils.weight_norm.WeightNorm object at 0x7f8034590a60>\n",
      "<torch.nn.utils.weight_norm.WeightNorm object at 0x7f8034592920>\n",
      "Done compiling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - For input x.1, found user specified input dtype as Float. The compiler is going to use the user setting Float\n"
     ]
    }
   ],
   "source": [
    "import torch_tensorrt\n",
    "import time\n",
    "import torch\n",
    "m=net(2,2)\n",
    "\n",
    "print(\"Begin compiling\")\n",
    "import torch_tensorrt\n",
    "\n",
    "X=np.zeros((2,5))\n",
    "\n",
    "\n",
    "model = torch.jit.script(m).eval()  # torch module needs to be in eval (not training) mode\n",
    "\n",
    "\n",
    "\n",
    "trt_ts_module = torch_tensorrt.compile(model, inputs = [torch_tensorrt.Input(\n",
    "        min_shape=(20,2),\n",
    "        opt_shape=(30,2),\n",
    "        max_shape=(40,2),\n",
    "        dtype=torch.float32)],\n",
    "    enabled_precisions = torch.float32, # Run with FP32\n",
    "    workspace_size = 1 << 33\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Done compiling\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def benchmark(model, input_shape=(1,2,1), dtype='fp32', nwarmup=50, nruns=10000):\n",
    "    input_data = torch.randn(input_shape)\n",
    "    input_data = input_data.to(\"cuda\")\n",
    "    if dtype=='fp16':\n",
    "        input_data = input_data.half()\n",
    "        \n",
    "    print(\"Warm up ...\")\n",
    "    with torch.no_grad():\n",
    "        for _ in range(nwarmup):\n",
    "            features = model(input_data)\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"Start timing ...\")\n",
    "    timings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(1, nruns+1):\n",
    "            start_time = time.time()\n",
    "            features = model(input_data)\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            timings.append(end_time - start_time)\n",
    "            if i%10==0:\n",
    "                print('Iteration %d/%d, ave batch time %.2f ms'%(i, nruns, np.mean(timings)*1000))\n",
    "                print('Images processed per second=', int(1000*input_shape[0]/(np.mean(timings)*1000)))\n",
    "    print(\"Input shape:\", input_data.size())\n",
    "    print(\"Output features size:\", features.size())\n",
    "    print('Average batch time: %.2f ms'%(np.mean(timings)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2cd14b1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3639280369.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[51], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(np.shape(my2dspace))aaa\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "X=np.zeros((35,2))\n",
    "print(np.shape(X))\n",
    "torch.cuda.empty_cache()\n",
    "  \n",
    "trt_ts_module.eval()\n",
    "my2dspace = pt.tensor(X, requires_grad=False).float().cuda()\n",
    "  \n",
    "print(np.shape(my2dspace))aaa\n",
    "myOutput = trt_ts_module(my2dspace)\n",
    "\n",
    "print(myOutput)\n",
    "print(\"Done inference\")\n",
    "\n",
    "\n",
    "benchmark(trt_ts_module,input_shape=(40,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c027e667",
   "metadata": {},
   "source": [
    "# import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import time as TIME\n",
    "import torch, gc\n",
    "\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "    \n",
    "t=np.linspace(0,10,5000)\n",
    "x=np.linspace(0,1,10000)\n",
    "X=np.zeros((2,len(t)*len(x)))\n",
    "    #print(itertools.product(x,t))\n",
    "\n",
    "\n",
    "model=Net(2,2,numlayers=4,H=4)\n",
    "PATH=\"\"\n",
    "od=pt.load('outputs/fhn1P/initial_conditions/network.0.pth')\n",
    "model.load(od)\n",
    "\n",
    "k=torch.jit.script(model).cuda()\n",
    "\n",
    "def eval(M):\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    #print(np.shape(X))\n",
    "    M.eval()\n",
    "    my2dspace = pt.tensor(X.T, requires_grad=False).float().cuda()\n",
    "   \n",
    "    start_time = TIME.time()\n",
    "\n",
    "\n",
    "    myOutput = M(my2dspace)\n",
    "    \n",
    "    reftime = TIME.time()- start_time\n",
    "    myCPUOutput = myOutput.cpu()\n",
    "\n",
    "\n",
    "    uu = myCPUOutput.numpy()\n",
    "\n",
    "    #print('uu: ', uu.T[0])\n",
    "\n",
    "    myCPUOutput.squeeze().detach().numpy()\n",
    "    #print(uu)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return reftime\n",
    "\n",
    "\n",
    "print(eval(k))\n",
    "print(eval(model))\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"oi\")\n",
    "import torch_tensorrt\n",
    "\n",
    "import torch\n",
    "m=net(2,2)\n",
    "\n",
    "print(m)\n",
    "\n",
    "\n",
    "trt_model = torch_tensorrt.compile(m, \n",
    "    inputs= [torch_tensorrt.Input((2,2))],\n",
    "    enabled_precisions= { torch_tensorrt.dtype.half} # Run with FP16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b0f047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
