{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83b3f822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.3246,  0.0120,  0.2973, -0.3162],\n",
      "        [ 0.3545,  0.0019,  0.0333, -0.1889],\n",
      "        [ 0.0741, -0.1634, -0.4797, -0.1625],\n",
      "        [ 0.2833, -0.1849, -0.4724, -0.2093],\n",
      "        [ 0.2217, -0.1538, -0.3769,  0.0140],\n",
      "        [ 0.4480,  0.1881,  0.1469, -0.3510]])), ('bias', tensor([ 0.2155,  0.3654,  0.1339,  0.3059, -0.4981,  0.0392]))])\n",
      "OrderedDict([('_impl.layers.0.linear.weight', tensor([[ 0.2217, -0.4893],\n",
      "        [-0.2422, -0.2661],\n",
      "        [-0.2954,  0.4892],\n",
      "        [-0.0252,  0.5018],\n",
      "        [ 0.0538, -0.5332],\n",
      "        [ 0.7230, -0.1501],\n",
      "        [ 0.3324, -0.4464],\n",
      "        [-0.1240, -0.3384],\n",
      "        [ 0.0773,  0.1993],\n",
      "        [ 0.3913,  0.5582],\n",
      "        [ 0.1043,  0.3439],\n",
      "        [ 0.2108, -0.2731],\n",
      "        [ 0.3459,  0.3473],\n",
      "        [ 0.4553,  0.2609],\n",
      "        [ 0.2451, -0.4571],\n",
      "        [-0.1408,  0.3125]], device='cuda:0')), ('_impl.layers.0.linear.weight_g', tensor([[1.0850],\n",
      "        [1.6957],\n",
      "        [2.9519],\n",
      "        [6.6868],\n",
      "        [2.7859],\n",
      "        [1.0233],\n",
      "        [1.2612],\n",
      "        [2.4525],\n",
      "        [1.8245],\n",
      "        [1.0652],\n",
      "        [1.0430],\n",
      "        [1.1403],\n",
      "        [0.9732],\n",
      "        [0.9920],\n",
      "        [1.5650],\n",
      "        [3.9624]], device='cuda:0')), ('_impl.layers.0.linear.bias', tensor([-0.0942,  1.8605, -0.2850, -0.8149, -0.5486, -1.2062,  0.4148,  1.5026,\n",
      "        -0.9008, -0.6633, -0.1255,  0.0441, -0.4225,  0.2856,  0.4562,  3.2535],\n",
      "       device='cuda:0')), ('_impl.layers.1.linear.weight', tensor([[ 2.2305e-01,  6.4573e-02,  2.1511e-01,  2.4473e-01, -9.4211e-02,\n",
      "          3.5646e-01,  2.9705e-01,  2.1003e-01,  3.0478e-01,  2.4981e-01,\n",
      "          2.5959e-01,  2.9118e-01,  1.4663e-01, -9.4727e-02, -3.0457e-01,\n",
      "         -5.3384e-01],\n",
      "        [-1.8812e-01,  5.3574e-01,  6.1291e-01,  2.7874e-01,  1.6160e-02,\n",
      "          4.1412e-01, -4.3586e-01,  2.8330e-01,  3.3270e-01,  2.3914e-02,\n",
      "         -2.4952e-02,  2.9006e-01,  2.4733e-01, -4.4362e-01,  1.4140e-01,\n",
      "         -2.7573e-02],\n",
      "        [ 3.6493e-02, -3.0777e-01, -4.2792e-02, -6.6737e-01, -9.0384e-02,\n",
      "          1.9044e-02,  4.3820e-01, -4.9743e-01, -3.6465e-01, -3.7226e-01,\n",
      "         -2.8477e-01, -1.0870e-01, -1.6805e-01,  2.6970e-01,  1.5436e-01,\n",
      "          2.5086e-01],\n",
      "        [-2.8417e-01, -3.9431e-01, -6.5208e-01,  5.8368e-01, -3.7448e-01,\n",
      "         -4.2542e-01, -1.8128e-01, -9.6017e-02,  2.0793e-01,  4.7257e-02,\n",
      "          1.6627e-01, -2.1522e-01,  2.1855e-01, -7.3644e-02, -2.0914e-01,\n",
      "          6.4787e-01],\n",
      "        [ 3.3539e-01,  5.0158e-01,  2.4324e-01,  2.9489e-01,  1.8461e-01,\n",
      "         -3.8145e-01,  7.4794e-03,  5.2744e-01,  8.6598e-02, -3.8187e-01,\n",
      "         -6.2499e-02,  4.7326e-02, -3.0191e-01,  3.3888e-01,  3.3757e-02,\n",
      "         -5.4445e-02],\n",
      "        [-1.8615e-01,  4.0460e-02, -8.5491e-02, -1.4391e+00,  1.5837e-01,\n",
      "         -2.8114e-01,  3.8271e-01, -3.0658e-01, -2.2373e-01,  9.2894e-02,\n",
      "         -3.9080e-01,  2.6427e-01, -2.7173e-01,  3.2082e-01,  3.0684e-01,\n",
      "         -8.1928e-01],\n",
      "        [ 2.1508e-02,  9.7628e-02, -1.5324e-01, -4.5166e-01, -1.3945e-01,\n",
      "          9.8591e-02,  1.3015e-01,  1.8389e-01, -2.6360e-01, -2.5072e-01,\n",
      "         -3.2949e-01,  2.2424e-01,  3.6616e-01,  3.1721e-01, -1.0857e-02,\n",
      "         -3.9286e-01],\n",
      "        [ 9.1758e-02, -4.0747e-01,  3.3204e-01, -6.5305e-01, -2.0690e-01,\n",
      "          5.3286e-05,  4.2221e-01, -6.1955e-01, -2.3757e-01, -2.9860e-02,\n",
      "          3.3987e-02,  8.9336e-02, -1.8448e-01,  2.9401e-01, -1.3761e-02,\n",
      "         -1.5290e-01],\n",
      "        [-2.3568e-01,  4.2358e-01, -2.9779e-01,  9.1885e-01, -6.9508e-02,\n",
      "          2.7951e-01, -2.4794e-01,  2.1959e-01,  1.4060e-01,  2.2999e-01,\n",
      "          1.0946e-01, -2.8668e-02,  1.3362e-01,  1.7876e-01, -2.5178e-01,\n",
      "         -4.0790e-01],\n",
      "        [-9.8390e-02,  1.3981e-01,  3.4286e-01,  2.3970e-01,  9.2280e-02,\n",
      "          3.3758e-01, -2.8530e-01,  2.6944e-01,  7.3612e-02,  3.6218e-01,\n",
      "         -2.4142e-01,  2.1306e-01,  1.7606e-01,  1.6942e-01, -2.7698e-01,\n",
      "         -3.1819e-01],\n",
      "        [ 2.4012e-02, -3.3449e-01,  1.1900e-01, -5.0868e-01, -9.5538e-02,\n",
      "         -1.6617e-01,  9.3852e-02, -4.0948e-01, -2.0486e-01, -1.0802e-01,\n",
      "         -3.4467e-02,  4.8947e-02, -3.8833e-02, -1.6544e-02,  1.0956e-01,\n",
      "          4.3360e-01],\n",
      "        [ 2.4483e-01, -2.2966e-01, -3.2600e-01, -2.8912e-01,  3.6029e-01,\n",
      "          3.5621e-01,  2.8100e-01,  3.5708e-01,  4.2376e-02,  3.1748e-01,\n",
      "         -1.0356e-01, -3.0177e-01, -3.8591e-02,  1.5383e-01, -3.9019e-01,\n",
      "         -3.2504e-01],\n",
      "        [ 3.7144e-02, -1.0873e-01,  4.2904e-01, -4.7181e-01, -1.4933e-01,\n",
      "          2.1294e-02,  1.5628e-01, -2.5799e-01, -2.6919e-01, -2.0733e-01,\n",
      "         -1.1640e-01,  6.8704e-02, -6.9331e-02, -3.4871e-02,  2.8000e-01,\n",
      "          2.6904e-01],\n",
      "        [-1.4090e-01,  1.9004e-01,  9.4028e-02,  1.9080e-01,  2.4845e-01,\n",
      "          1.0833e-01,  2.3488e-01, -3.4997e-01,  2.9729e-01,  2.9880e-01,\n",
      "         -7.2528e-02, -3.8929e-01,  2.8520e-01, -2.0206e-03,  7.6855e-02,\n",
      "          2.6807e-01],\n",
      "        [ 3.5328e-02, -5.8243e-01, -8.6572e-02, -2.0792e-01, -1.6838e-01,\n",
      "         -1.2567e-01,  8.5390e-02, -6.1351e-02, -7.5460e-02, -1.4197e-01,\n",
      "          1.0775e-01,  2.6684e-02, -1.5249e-01,  1.6504e-01, -1.1029e-01,\n",
      "          4.3817e-01],\n",
      "        [-4.1063e-01, -4.4838e-01, -5.5118e-02,  2.5377e-01, -1.2348e-01,\n",
      "         -1.9876e-01, -5.5877e-02, -3.0359e-01,  1.3077e-02,  1.1826e-01,\n",
      "         -1.5180e-01,  2.0456e-01, -1.4444e-01,  1.1478e-01,  9.9792e-02,\n",
      "          3.9773e-01]], device='cuda:0')), ('_impl.layers.1.linear.weight_g', tensor([[1.2051],\n",
      "        [1.2264],\n",
      "        [2.2092],\n",
      "        [1.7990],\n",
      "        [1.2850],\n",
      "        [2.7245],\n",
      "        [1.1206],\n",
      "        [2.1539],\n",
      "        [1.6320],\n",
      "        [1.0559],\n",
      "        [3.7572],\n",
      "        [1.0120],\n",
      "        [3.9442],\n",
      "        [0.9929],\n",
      "        [1.6701],\n",
      "        [1.1018]], device='cuda:0')), ('_impl.layers.1.linear.bias', tensor([-0.1018, -0.3620,  0.5505,  0.1507,  0.0033,  0.7843,  0.2044,  0.9316,\n",
      "        -0.2008, -0.1156,  1.4514, -0.1483,  0.7728,  0.0421,  0.3312,  0.0743],\n",
      "       device='cuda:0')), ('_impl.layers.2.linear.weight', tensor([[ 0.4983,  0.0361, -0.3024,  0.1829, -0.1841, -0.0902, -0.2423, -0.4933,\n",
      "          0.3903, -0.0155, -0.1827,  0.1227,  0.1982, -0.1941, -0.1463, -0.0941],\n",
      "        [ 0.2639, -0.1542, -0.2273,  0.2259, -0.3785, -0.2048,  0.2423, -0.2226,\n",
      "         -0.1920, -0.2599, -0.3805,  0.1225,  0.1094,  0.3710,  0.1166,  0.3837],\n",
      "        [-0.2088, -0.3824,  0.0902,  1.0640, -0.4233, -0.3433, -0.0702, -0.2164,\n",
      "          0.0960, -0.6004, -0.4562, -0.1555,  0.4878, -0.2676, -0.2170,  0.0970],\n",
      "        [-0.5722, -0.2537, -0.1011,  0.3085, -0.2429,  0.0813, -0.2889,  0.2205,\n",
      "         -0.4729, -0.2442, -0.6191, -0.2643, -0.1950, -0.0184,  0.3507,  0.1704],\n",
      "        [ 0.1151, -0.0810, -0.1672, -0.1355, -0.1642,  0.0020, -0.0874, -0.3170,\n",
      "          0.2775,  0.1704,  0.3396, -0.1789,  0.1571,  0.4195,  0.0103,  0.2873],\n",
      "        [-0.3593, -0.1965, -0.2300,  0.1301, -0.3173, -0.1716, -0.2643, -0.0758,\n",
      "         -0.1619, -0.2584, -0.5180, -0.4115, -0.3276,  0.2671,  0.2555,  0.3182],\n",
      "        [ 0.1538, -0.3725,  0.1941, -0.1841,  0.4178, -0.3898, -0.0837, -0.0620,\n",
      "          0.1612,  0.0551,  0.1467, -0.0452, -0.2756,  0.2911, -0.2624, -0.2580],\n",
      "        [ 0.2229,  0.0405,  0.2781, -0.1347, -0.0176, -0.2722,  0.1183, -0.2475,\n",
      "          0.0970, -0.3186,  0.1861,  0.3770, -0.3515,  0.4494, -0.3023,  0.0026],\n",
      "        [-0.0620,  0.0431, -0.2966, -0.7482, -0.0453, -0.2020,  0.0735,  0.2360,\n",
      "         -0.3271, -0.0161, -0.0317,  0.0605,  0.0832, -0.1578,  0.1588, -0.2628],\n",
      "        [-0.1773,  0.2150,  0.2639, -0.2736,  0.1848, -0.4487,  0.2488,  0.5307,\n",
      "         -0.5695, -0.2061,  0.0600,  0.1767, -0.1965,  0.0502,  0.2593,  0.1652],\n",
      "        [ 0.0942, -0.2109,  0.2453, -0.1621,  0.0146, -0.0415,  0.2303, -0.2711,\n",
      "          0.5020,  0.3219,  0.2131, -0.1947, -0.1777,  0.2115, -0.3228, -0.3016],\n",
      "        [ 0.0681, -0.3080, -0.1990, -0.3495, -0.1760,  0.0258,  0.0164,  0.0363,\n",
      "          0.4486, -0.1004,  0.3743, -0.1380, -0.0584,  0.1129, -0.2240, -0.0228],\n",
      "        [ 0.0462, -0.1353,  0.5819, -0.0823, -0.3314, -0.0513, -0.3986,  0.4100,\n",
      "          0.2311, -0.6717,  0.3708, -0.5534, -0.3806, -0.3979, -0.2893,  0.3434],\n",
      "        [ 0.1594,  0.0847, -0.2927,  0.1593, -0.2536, -0.1384, -0.2458,  0.1547,\n",
      "         -0.5531, -0.4274,  0.1090,  0.1289, -0.4842, -0.0860,  0.2554,  0.1364],\n",
      "        [-0.1975,  0.1776,  0.3634, -0.5078, -0.2674,  0.1143, -0.0774,  0.3706,\n",
      "          0.0069, -0.1635,  0.3846, -0.3353, -0.2059,  0.0747, -0.3001, -0.4672],\n",
      "        [-0.4128, -0.0255,  0.1961,  0.2192, -0.4447,  0.1114,  0.3442,  0.1298,\n",
      "          0.3980,  0.2585,  0.1217, -0.0471,  0.1200, -0.3833, -0.1677, -0.0829]],\n",
      "       device='cuda:0')), ('_impl.layers.2.linear.weight_g', tensor([[1.0512],\n",
      "        [0.9659],\n",
      "        [1.0385],\n",
      "        [1.9270],\n",
      "        [1.0288],\n",
      "        [1.1471],\n",
      "        [0.8789],\n",
      "        [1.0379],\n",
      "        [1.9799],\n",
      "        [8.1835],\n",
      "        [1.0213],\n",
      "        [1.4281],\n",
      "        [0.9808],\n",
      "        [1.1493],\n",
      "        [1.2405],\n",
      "        [1.2663]], device='cuda:0')), ('_impl.layers.2.linear.bias', tensor([-0.0288, -0.1068, -0.1556,  0.4333,  0.0605,  0.2425, -0.2198,  0.0164,\n",
      "         0.2870,  1.2458,  0.1139,  0.3063,  0.0893,  0.4293, -0.1038, -0.1014],\n",
      "       device='cuda:0')), ('_impl.layers.3.linear.weight', tensor([[-8.6431e-02, -2.7449e-01,  1.5766e-01,  3.7847e-01,  1.5760e-01,\n",
      "         -1.2172e-01, -3.2816e-01,  1.0903e-01, -4.2647e-01,  4.7616e-01,\n",
      "         -1.5834e-01,  1.0500e-01,  4.2156e-01,  3.5601e-01, -9.0247e-02,\n",
      "         -1.2849e-01],\n",
      "        [-2.3352e-02, -4.7054e-01, -5.1726e-01, -1.2407e-01, -3.6766e-01,\n",
      "         -1.9084e-01, -4.0920e-04,  3.9690e-01, -1.8541e-01, -1.9029e-01,\n",
      "         -1.5290e-01, -3.9382e-01,  4.1024e-01,  4.8544e-01, -2.9946e-01,\n",
      "          1.0757e-01],\n",
      "        [-9.4641e-02,  3.0940e-01, -1.8721e-01,  2.4128e-02,  1.2770e-01,\n",
      "          3.4627e-01, -4.1734e-01, -6.0598e-02, -4.1200e-01, -5.5299e-01,\n",
      "         -1.5149e-01, -5.8991e-01,  4.9169e-01, -2.0155e-01,  5.0123e-01,\n",
      "         -4.3074e-02],\n",
      "        [-3.9238e-01, -1.0185e-01,  3.3810e-02, -1.1149e-01,  2.6248e-01,\n",
      "          4.1694e-01, -4.1596e-02,  1.4754e-01, -3.7435e-02,  1.6309e-02,\n",
      "          1.8600e-01, -2.9578e-01, -5.0782e-02, -3.3440e-01,  5.6154e-01,\n",
      "         -1.9673e-01],\n",
      "        [-4.5983e-01, -3.5114e-01, -5.6888e-02,  1.6947e-01,  2.0175e-01,\n",
      "          2.1116e-01,  3.4372e-01, -7.7703e-02,  1.1334e-01,  1.8035e-01,\n",
      "          4.7210e-04,  1.7767e-02,  3.0280e-01,  3.0864e-01,  2.1350e-01,\n",
      "         -1.9711e-01],\n",
      "        [-1.6161e-01,  1.2577e-01, -1.8746e-01, -2.6849e-01, -2.2407e-01,\n",
      "          5.9007e-01, -1.7335e-01,  1.5326e-01, -2.2428e-01, -5.8093e-01,\n",
      "         -1.8118e-01, -5.4432e-01, -1.9083e-02,  8.7247e-02, -2.5602e-02,\n",
      "          2.6126e-01],\n",
      "        [-1.6445e-01,  2.9232e-01, -3.8787e-01, -5.9322e-01, -1.5973e-01,\n",
      "          3.2749e-01,  3.0131e-01, -1.8647e-01, -2.7683e-01, -2.6680e-01,\n",
      "         -2.8824e-02, -3.2122e-01,  4.6165e-01, -3.4670e-01,  9.9538e-02,\n",
      "          2.9801e-01],\n",
      "        [-3.4417e-01,  1.0861e-01,  3.0390e-01, -3.9666e-01, -3.8801e-01,\n",
      "          2.9922e-01,  2.0127e-01, -2.4878e-01,  3.7438e-01, -9.8624e-02,\n",
      "         -3.0898e-01,  1.7343e-01, -1.8162e-01,  2.7937e-01,  9.0695e-02,\n",
      "          1.3938e-01],\n",
      "        [ 3.9430e-01, -2.8942e-01, -2.7780e-02, -6.4295e-01, -3.1159e-01,\n",
      "         -5.5533e-01,  1.7712e-01, -2.0018e-01, -1.8163e-01, -6.6667e-03,\n",
      "         -1.3705e-01,  1.7534e-01, -2.8630e-01, -1.1712e+00,  4.3131e-02,\n",
      "          3.7620e-01],\n",
      "        [ 6.9588e-02, -2.0846e-01, -2.2729e-01,  1.3782e-01,  2.2798e-02,\n",
      "         -1.6640e-01,  5.3315e-02,  4.0404e-01, -2.4378e-01, -4.5856e-01,\n",
      "         -2.2147e-01, -4.1595e-01, -5.1092e-01,  1.0576e-01,  3.2648e-01,\n",
      "          4.6395e-01],\n",
      "        [-4.8541e-02,  9.6117e-02, -1.5579e-02,  3.6694e-01,  2.5148e-01,\n",
      "         -3.3760e-01, -1.0060e-01, -4.5244e-01, -1.5389e-01, -1.9897e-01,\n",
      "          1.0867e-01,  6.8868e-02,  1.2738e-01,  3.6130e-01,  4.7311e-01,\n",
      "         -4.0693e-02],\n",
      "        [ 3.3270e-01, -3.2879e-01, -8.5577e-02,  4.2703e-01, -4.7831e-02,\n",
      "         -4.5004e-01, -3.4593e-01, -8.9914e-02, -3.8093e-02, -1.0527e-01,\n",
      "         -1.6830e-01,  3.1499e-01, -1.3042e-01,  5.1047e-01,  1.7626e-01,\n",
      "          2.4654e-01],\n",
      "        [-6.4299e-02, -4.2584e-01, -2.2891e-01, -2.2792e-01, -3.5655e-01,\n",
      "         -1.7907e-01,  5.7163e-02, -4.7882e-01, -9.0071e-02, -1.6650e-01,\n",
      "          3.2307e-01, -3.6623e-01,  1.8283e-01, -5.2825e-01,  2.6180e-01,\n",
      "          3.2540e-01],\n",
      "        [-7.2961e-02, -3.8682e-03,  4.4474e-02, -1.0092e-02, -4.3013e-01,\n",
      "          1.3301e-01,  7.1479e-02, -2.1022e-01, -5.3662e-01, -1.3859e-01,\n",
      "         -4.0914e-01,  1.0160e-01,  5.3331e-01,  8.0448e-02,  2.5047e-02,\n",
      "         -9.6035e-02],\n",
      "        [-2.3661e-01,  3.4719e-01, -1.2082e-01, -1.2442e-01,  1.2431e-01,\n",
      "         -2.1504e-01, -1.4915e-01, -3.6562e-01, -4.3310e-01,  2.7520e-01,\n",
      "          3.3453e-01, -2.2465e-01, -5.6253e-03, -1.6618e-01, -2.7402e-01,\n",
      "         -2.8316e-01],\n",
      "        [-1.9514e-01,  4.1012e-01, -9.7385e-02,  5.2015e-02, -2.1351e-01,\n",
      "          1.3507e-01,  3.5717e-01, -5.6285e-02,  1.6399e-01, -4.9214e-01,\n",
      "         -1.1878e-01, -5.1352e-01, -1.8061e-02, -7.4903e-02,  2.0275e-01,\n",
      "          8.9128e-02]], device='cuda:0')), ('_impl.layers.3.linear.weight_g', tensor([[0.8758],\n",
      "        [1.2504],\n",
      "        [1.1468],\n",
      "        [0.9883],\n",
      "        [0.8787],\n",
      "        [1.8180],\n",
      "        [1.0154],\n",
      "        [1.4406],\n",
      "        [1.4940],\n",
      "        [0.8024],\n",
      "        [0.8990],\n",
      "        [0.9185],\n",
      "        [1.0250],\n",
      "        [1.0686],\n",
      "        [1.0406],\n",
      "        [1.1781]], device='cuda:0')), ('_impl.layers.3.linear.bias', tensor([-0.0355, -0.0019, -0.0377,  0.1212,  0.0029,  0.0088, -0.0670,  0.1349,\n",
      "        -0.3480, -0.0270, -0.0147,  0.0845, -0.1482, -0.0625,  0.0351, -0.0019],\n",
      "       device='cuda:0')), ('_impl.final_layer.linear.weight', tensor([[-0.2229, -0.5551,  0.4636,  0.2677,  0.3847,  0.5302,  0.3408,  0.4944,\n",
      "         -0.3149, -0.0394,  0.1839, -0.0764, -0.0048, -0.3374,  0.0114, -0.0436],\n",
      "        [ 0.3245, -0.3047,  0.1726, -0.0516, -0.4866,  0.2085,  0.4867, -0.1898,\n",
      "          0.2089, -0.1823,  0.1532, -0.1761, -0.3858,  0.4403, -0.1365, -0.4734]],\n",
      "       device='cuda:0')), ('_impl.final_layer.linear.bias', tensor([ 0.0728, -0.0287], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "loaded_module = nn.Linear(4,6)\n",
    "#loaded_module.load_state_dict()\n",
    "\n",
    "print(loaded_module.state_dict())\n",
    "st=torch.load('outputs/fhn1P/initial_conditions/network.0.pth')\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6c8bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as pt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "import collections as coll\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch as pt\n",
    "\n",
    "pt.set_grad_enabled (False) \n",
    "numinputs=1\n",
    "numoutputs=2\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, numinputs, numoutputs, numlayers=4, H=10):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.utils.weight_norm(nn.Linear(numinputs, H), name='weight', dim=0).cuda())\n",
    "\n",
    "        for _ in range(numlayers - 1):\n",
    "            self.layers.append(nn.utils.weight_norm(nn.Linear(H, H), name='weight', dim=0).cuda())\n",
    "\n",
    "        self.final_layer = nn.Linear(H, numoutputs).cuda()\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.eval()\n",
    "        self.final_layer.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = f.silu(layer(x))\n",
    "\n",
    "        return self.final_layer(x)\n",
    "\n",
    "    def load(self, od):\n",
    "        for k, v in od.items():\n",
    "            if k.startswith('_impl.layers'):\n",
    "                layer_num = int(k.split('.')[2])\n",
    "                layer = self.layers[layer_num]\n",
    "                if k.endswith('linear.weight'):\n",
    "                    layer.weight_v.data = v\n",
    "                    layer.weight_v.requires_grad = False\n",
    "                elif k.endswith('linear.weight_g'):\n",
    "                    layer.weight_g.data = v\n",
    "                    layer.weight_g.requires_grad = False\n",
    "                elif k.endswith('linear.bias'):\n",
    "                    layer.bias.data = v\n",
    "                    layer.bias.requires_grad = False\n",
    "            elif k == '_impl.final_layer.linear.weight':\n",
    "                self.final_layer.weight.data = v\n",
    "                self.final_layer.weight.requires_grad = False\n",
    "            elif k == '_impl.final_layer.linear.bias':\n",
    "                self.final_layer.bias.data = v\n",
    "                self.final_layer.bias.requires_grad = False\n",
    "\n",
    "    def __prepare_scriptable__(self):\n",
    "        for layer in self.layers:\n",
    "            for hook in layer._forward_pre_hooks.values():\n",
    "                if hook.__module__ == \"torch.nn.utils.weight_norm\" and hook.__class__.__name__ == \"WeightNorm\":\n",
    "                    torch.nn.utils.remove_weight_norm(layer)\n",
    "        return self\n",
    "\n",
    "import itertools\n",
    "def ModelrunRT(x=np.linspace(0, 1, num=10),t=np.linspace(0,10,num=10)):\n",
    "   \n",
    "    \n",
    "    X=np.zeros((2,len(t)*len(x)))\n",
    "    #print(itertools.product(x,t))\n",
    "    i=0\n",
    "    for a,b in itertools.product(x,t):\n",
    "        X[:,i]=(b,a)\n",
    "        i=i+1\n",
    "    #print(np.shape(X))\n",
    "    my2dspace = pt.tensor(X.T, requires_grad=False)\n",
    "    model.eval()\n",
    "\n",
    "    myOutput = model(my2dspace.float().cuda())\n",
    "    myCPUOutput = myOutput.cpu()\n",
    "\n",
    "\n",
    "    uu = myCPUOutput.numpy()\n",
    "\n",
    "    #print('uu: ', uu.T[0])\n",
    "\n",
    "    myCPUOutput.squeeze().detach().numpy()\n",
    "    return uu\n",
    "\n",
    "\n",
    "\n",
    "def Modelrun(x=np.linspace(0, 1, num=10),t=np.linspace(0,10,num=10),M=0):\n",
    "   \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    X=np.zeros((2,len(t)*len(x)))\n",
    "    #print(itertools.product(x,t))\n",
    "    i=0\n",
    "    for a,b in itertools.product(x,t):\n",
    "        X[:,i]=(b,a)\n",
    "        i=i+1\n",
    "    #print(np.shape(X))\n",
    "    my2dspace = pt.tensor(X.T, requires_grad=False).float().cuda()\n",
    "    M.eval()\n",
    "    start_time = TIME.time()\n",
    "    myOutput = M(my2dspace)\n",
    "    reftime = TIME.time()- start_time\n",
    " \n",
    "    myCPUOutput = myOutput.cpu()\n",
    "\n",
    "\n",
    "    uu = myCPUOutput.numpy()\n",
    "\n",
    "    #print('uu: ', uu.T[0])\n",
    "\n",
    "    myCPUOutput.squeeze().detach().numpy()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return uu,reftime\n",
    "\n",
    "def Modelrunrt(x=np.linspace(0, 1, num=10),t=np.linspace(0,10,num=10),M=0):\n",
    "   \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    X=np.zeros((2,len(t)*len(x)))\n",
    "    #print(itertools.product(x,t))\n",
    "    i=0\n",
    "    for a,b in itertools.product(x,t):\n",
    "        X[:,i]=(b,a)\n",
    "        i=i+1\n",
    "    #print(np.shape(X))\n",
    "   \n",
    "    my2dspace = pt.tensor(X, requires_grad=False).float().cuda()\n",
    "    M.eval()\n",
    "    start_time = TIME.time()\n",
    "    myOutput = M(my2dspace)\n",
    "    reftime = TIME.time()- start_time\n",
    " \n",
    "    myCPUOutput = myOutput.cpu()\n",
    "\n",
    "\n",
    "    uu = myCPUOutput.numpy()\n",
    "\n",
    "    #print('uu: ', uu.T[0])\n",
    "\n",
    "    myCPUOutput.squeeze().detach().numpy()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return uu,reftime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ab03a2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T17:53:33.467022Z",
     "iopub.status.busy": "2023-07-02T17:53:33.466776Z",
     "iopub.status.idle": "2023-07-02T17:53:36.561448Z",
     "shell.execute_reply": "2023-07-02T17:53:36.560668Z",
     "shell.execute_reply.started": "2023-07-02T17:53:33.467000Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvcc cuda.cu -o a.out  ##Estava seguindo o tutorioal do torch rt tentando encontrar uma forma de converter, tentei remover os pesos para entÃ£o converte "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60e58815",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[0;32m----> 6\u001b[0m net \u001b[38;5;241m=\u001b[39m\u001b[43mNet\u001b[49m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      7\u001b[0m PATH\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m od\u001b[38;5;241m=\u001b[39mpt\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs/fhn1P/initial_conditions/network.0.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Net' is not defined"
     ]
    }
   ],
   "source": [
    "import torch_tensorrt\n",
    "\n",
    "from FHNCUDAlib import FHNCUDA\n",
    "import numpy as np\n",
    "import torch \n",
    "net =Net(2,2)\n",
    "PATH=\"\"\n",
    "od=pt.load('outputs/fhn1P/initial_conditions/network.0.pth')\n",
    "net.load(od)\n",
    "net=net.cuda()\n",
    "\n",
    "\n",
    "pt.no_grad() \n",
    "import time as TIME\n",
    "x0=np.expand_dims(np.array([(0 + 0.00001*i*1) for i in range(0,100)]),-1)\n",
    "dt,tt=0.0001,10\n",
    "\n",
    "rate=4000\n",
    "\n",
    "start_time = TIME.time()\n",
    "u,v ,t,p=FHNCUDA.run(x0,tt,dt,rate)\n",
    "reftime = TIME.time()- start_time\n",
    "p=[i/1000 for i in p[0]]\n",
    "u_ref=np.array(u).flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start_time = TIME.time()\n",
    "u,v ,t,p=FHNCUDA.run(x0,tt,dt*100,rate/100)\n",
    "cudatime = TIME.time()- start_time\n",
    "u_num=np.array(u).flatten()\n",
    "#print(np.unique(t))\n",
    "p=[i/1000 for i in p[0]]\n",
    "print(\"Shape cudapred \",np.shape(u_num))\n",
    "print(\"cuda time\",p)\n",
    "print(\"Error Calculation\")\n",
    "e=((u_ref-u_num)**2)**(1/2)\n",
    "print(\"mean\",np.mean(e))\n",
    "m=np.max(e)\n",
    "print(\"max\",m)\n",
    "\n",
    "i=[a for a in range(len(e)) if e[a]==m]\n",
    "x0 = [item for sublist in x0 for item in sublist]\n",
    "t = [item for sublist in t for item in sublist]\n",
    "\n",
    "print(np.shape(t))\n",
    "start_time = TIME.time()\n",
    "u,net_time=Modelrun(x0,t[1:],M=net)\n",
    "\n",
    "u_net=u.T[0].flatten()\n",
    "\n",
    "print(\"Shape netpred \",np.shape(u_net))\n",
    "print(\"net time\", net_time)\n",
    "print(\"Error Calculation\")\n",
    "e=((u_ref-u_net)**2)**(1/2)\n",
    "print(\"mean\",np.mean(e))\n",
    "m=np.max(e)\n",
    "print(\"max\",m)\n",
    "i=[a for a in range(len(e)) if e[a]==m]\n",
    "\n",
    "\n",
    "plt.plot(u_ref,\"b\")\n",
    "plt.plot(u_net,\"r\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(np.shape(t))\n",
    "start_time = TIME.time()\n",
    "\n",
    "u,net_time=Modelrun(x0,t[1:],M=torch.jit.script(net).cuda())\n",
    "\n",
    "\n",
    "\n",
    "u_net=u.T[0].flatten()\n",
    "\n",
    "print(\"Shape netpred \",np.shape(u_net))\n",
    "print(\"net time\", net_time)\n",
    "print(\"Error Calculation\")\n",
    "e=((u_ref-u_net)**2)**(1/2)\n",
    "print(\"mean\",np.mean(e))\n",
    "m=np.max(e)\n",
    "print(\"max\",m)\n",
    "i=[a for a in range(len(e)) if e[a]==m]\n",
    "\n",
    "\n",
    "plt.plot(u_ref,\"b\")\n",
    "plt.plot(u_net,\"r\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad7f6622",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-02T17:59:11.837122Z",
     "iopub.status.busy": "2023-07-02T17:59:11.836808Z",
     "iopub.status.idle": "2023-07-02T17:59:14.688048Z",
     "shell.execute_reply": "2023-07-02T17:59:14.687318Z",
     "shell.execute_reply.started": "2023-07-02T17:59:11.837087Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvcc cuda.cu -o a.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb776836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
